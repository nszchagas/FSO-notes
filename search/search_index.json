{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fundamentos de Sistemas Operacionais","text":"<p>P\u00e1gina dedicada aos resumos elaborados durante a disciplina de Fundamentos de Sistemas Operacionais da UnB, durante o 1\u00ba semestre de 2023.</p> <p>Todos os resumos baseiam-se, principalmente, no conte\u00fado dos livros:</p> <ul> <li>SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition.</li> <li>TANENBAUM, A. S. Sistemas. Operacionais Modernos. 4\u00aa ed. Prentice Hall, 2016.</li> </ul>"},{"location":"resumos/mod1/","title":"M\u00f3dulo 1","text":""},{"location":"resumos/mod1/#referencias","title":"Refer\u00eancias","text":"<p>[1] TANENBAUM, A. S. Sistemas. Operacionais Modernos. 4\u00aa ed. Prentice Hall, 2016.</p>"},{"location":"resumos/mod2/","title":"Processos","text":"<p>Um processo \u00e9 um programa em execu\u00e7\u00e3o acompanhado de valores de tempo de execu\u00e7\u00e3o. \u00c9 composto por:</p> <ul> <li>C\u00f3digo execut\u00e1vel</li> <li>Pilha de execu\u00e7\u00e3o contendo valores de vari\u00e1veis locais</li> <li>Apontador para a pilha, um registrador da CPU que armazena em qual \u00e1rea de mem\u00f3ria est\u00e1 a pilha</li> <li>Contador de programa, um registrador da CPU que armazena a pr\u00f3xima instru\u00e7\u00e3o a ser executada</li> <li>Valores dos registradores gerais da m\u00e1quina</li> </ul> <p>Note a diferen\u00e7a entre programas e processos: um processo envolve uma atividade, enquanto programas envolvem algoritmos, mas n\u00e3o a sua execu\u00e7\u00e3o.</p> <p>Para um processo, s\u00e3o relevantes informa\u00e7\u00f5es sobre o ambiente, como espa\u00e7o de endere\u00e7amento, arquivos abertos, processos filhos, sinais e estat\u00edsticas de uso, e sobre a execu\u00e7\u00e3o, como o contador de programa, apontador de pilha, conjunto de registradores e estado de execu\u00e7\u00e3o.</p> <p>Os processos podem ser classificados em rela\u00e7\u00e3o ao custo da troca de contexto e manuten\u00e7\u00e3o em heavyweight (processo tradicional) ou lightweight (threads). O processo heavyweight \u00e9 composto tanto pelo ambiente quanto pela execu\u00e7\u00e3o. Cada processo possui um \u00fanico fluxo de controle, contador de programa, e roda de forma independente dos demais. Em um dado momento h\u00e1 v\u00e1rios processos ativos e o processador \u00e9 chaveado entre eles.</p> <p> <p></p> <p></p>"},{"location":"resumos/mod2/#ciclo-de-vida-dos-processos","title":"Ciclo de Vida dos Processos","text":""},{"location":"resumos/mod2/#criacao","title":"Cria\u00e7\u00e3o","text":"<p>Processos podem ser criados:</p> <ul> <li>No in\u00edcio do sistema;</li> <li>Ao executar uma chamada ao sistema de cria\u00e7\u00e3o de processo por um processo em execu\u00e7\u00e3o;</li> <li>Por requisi\u00e7\u00e3o do usu\u00e1rio;</li> <li>In\u00edcio de um job de lote.</li> </ul> <p>Ao iniciar o sistema operacional, diversos processos s\u00e3o criados, alguns executando em primeiro plano e interagindo com o usu\u00e1rio, outros em segundo plano, chamados de daemons. Normalmente os processos s\u00e3o criados por outro.</p> <p> <p> </p> <p>No Unix h\u00e1 apenas uma forma de se criar processos, por meio da clonagem (<code>fork()</code>). Esse comando cria um clone id\u00eantico ao processo que o chamou, e normalmente \u00e9 executado <code>execve</code> depois da chamada para mudar o \"programa\" em execu\u00e7\u00e3o. No Windows h\u00e1 uma fun\u00e7\u00e3o que trata o processo de cria\u00e7\u00e3o e carga do programa, o <code>CreateProcess</code>.</p>"},{"location":"resumos/mod2/#termino","title":"T\u00e9rmino","text":"<p>Processos podem terminar de forma volunt\u00e1ria ou involunt\u00e1ria, com as seguintes condi\u00e7\u00f5es de t\u00e9rmino:</p> <ul> <li>Sa\u00edda normal (volunt\u00e1ria);</li> <li>Sa\u00edda por erro (volunt\u00e1ria);</li> <li>Erro fatal (involunt\u00e1rio);</li> <li>Cancelamento por outro processo (involunt\u00e1rio);</li> </ul>"},{"location":"resumos/mod2/#hierarquia","title":"Hierarquia","text":"<p>Quando um processo \u00e9 criado por outro, eles continuam associados, e em alguns sistemas \u00e9 poss\u00edvel listar o <code>ppid</code> (parent pid) de um processo.</p> <p>No Unix todos os processos s\u00e3o filhos do <code>init</code> (ou <code>systemd</code>). O Windows n\u00e3o apresenta hierarquia de processos.</p>"},{"location":"resumos/mod2/#estados","title":"Estados","text":"<p>Quando um processo est\u00e1 esperando um evento, como leitura em disco, leitura de rede ou entrada do usu\u00e1rio, dizemos que o processo est\u00e1 bloqueado. Um processo pode estar:</p> <ul> <li>Rodando</li> <li>Bloqueado</li> <li>Pronto</li> </ul> <p>Sistemas monoprocessados s\u00e3o aqueles que possuem apenas um processo rodando.</p> <p> <p> </p> <p>Um processo circula entre os estados conforme ilustrado, de forma que quando o processo aguarda um evento ele se bloqueia (1), quando o evento esperado ocorre, o processo torna-se pronto (2), ent\u00e3o pode ser escolhido pelo escalonador para executar (4) e retorna para o estado de pronto quando seu tempo de posse do processador se esgota (3).</p>"},{"location":"resumos/mod2/#classificacao","title":"Classifica\u00e7\u00e3o","text":""},{"location":"resumos/mod2/#processos-cpu-bound-e-io-bound","title":"Processos CPU Bound e I/O Bound","text":"<p>Processos podem ser classificados como CPU ou I/O bound:</p> <ul> <li>CPU Bound (afinidade \u00e0 CPU): passam a maior parte do tempo usando a CPU, no estado rodando ou pronto.</li> <li>I/O Bound (afinidade \u00e0 Entrada e Sa\u00edda): passam a maior parte do tempo em estado bloqueado por causarem muitas opera\u00e7\u00f5es de entrada e sa\u00edda.</li> </ul>"},{"location":"resumos/mod2/#implementacao","title":"Implementa\u00e7\u00e3o","text":"<p>Todas as informa\u00e7\u00f5es sobre um processo s\u00e3o mantidas na tabela de processos (ou bloco de controle de processo), que pode ser acessada pelo comando <code>ps aux</code>. Os campos dizem respeito \u00e0 ger\u00eancia do processo, da mem\u00f3ria e dos arquivos. Cada processo possui um identificador \u00fanico <code>pid</code> (process id).</p> <p>Na tabela de processos, constam as seguintes informa\u00e7\u00f5es:</p>  Gerenciamento de processos  Gerenciamento de mem\u00f3ria  Gerenciamento de arquivos  Registradores                  Ponteiro para o segmento de dados Diret\u00f3rio-raiz                  Contador de programa            Palava de estado do programa    Ponteiro de pilha             Diret\u00f3rio de trabalho            Estado do processo              Prioridade                     Ponteiro para o segmento de pilha   Par\u00e2metros de escalonamento   Descritores de arquivos              Identificador do processo(PID)  Processo pai (PPID)             Grupo do processo             Identificador do usu\u00e1rio             Sinais                          Momento em que iniciou         Ponteiro para o segmento de c\u00f3digo  Tempo de uso de CPU           Identificador do grupo                Tempo de CPU do filho           Momento do pr\u00f3ximo alarme      <p>A troca de contexto \u00e9 a opera\u00e7\u00e3o de salvar os registradores de um processo e restaura\u00e7\u00e3o dos registradores de outro. A troca de contexto permite a troca do processador entre os processos e \u00e9 a opera\u00e7\u00e3o b\u00e1sica da multiprograma\u00e7\u00e3o.</p> <p>Para manter a ilus\u00e3o de m\u00faltiplos processos rodando sequencialmente em uma m\u00e1quina com uma CPU e v\u00e1rios dispositivos de E/S, esses dispositivos possuem uma \u00e1rea da mem\u00f3ria chamada vetor de interrup\u00e7\u00f5es, que cont\u00e9m os endere\u00e7os de procedimentos dos servi\u00e7os de interrup\u00e7\u00e3o.</p>"},{"location":"resumos/mod2/#escalonamento-de-processos","title":"Escalonamento de Processos","text":"<p>O Sistema Operacional \u00e9 respons\u00e1vel por gerenciar os recursos de processamento de um computador, e essa atividade \u00e9 conhecida como escalonamento de processador. J\u00e1 a intera\u00e7\u00e3o entre processos \u00e9 realizada por meio de mecanismos de comunica\u00e7\u00e3o.</p> <p>Quando m\u00faltiplos processos encontram-se no estado pronto, o escalonador do sistema operacional elege um para execu\u00e7\u00e3o, utilizando um algoritmo de escalonamento. Este algoritmo \u00e9 respons\u00e1vel por determinar qual processo ir\u00e1 rodar, e por quanto tempo poder\u00e1 utilizar o processador.</p> <p>Quando um processo solicita opera\u00e7\u00f5es blocantes (E/S), sua execu\u00e7\u00e3o fica suspensa at\u00e9 que o evento ocorra.</p> <p>A execu\u00e7\u00e3o concorrente prov\u00e9m uma melhor utiliza\u00e7\u00e3o da CPU, especialmente para processos I/O Bound.</p> <p> <p> </p> <p>Os escalonadores podem ser classificados em preemptivos e n\u00e3o-preemptivos*, sendo que a preemp\u00e7\u00e3o \u00e9 a suspens\u00e3o tempor\u00e1ria da execu\u00e7\u00e3o de um processo.</p>"},{"location":"resumos/mod2/#classificacao-de-escalonadores","title":"Classifica\u00e7\u00e3o de Escalonadores","text":""},{"location":"resumos/mod2/#escalonadores-nao-preemptivos","title":"Escalonadores N\u00e3o-Preemptivos","text":"<p>Os escalonadores n\u00e3o-preemptivos permitem que um processo rode at\u00e9 o fim, ou at\u00e9 que ele mesmo se bloqueie, ap\u00f3s a obten\u00e7\u00e3o do processador. Nessa situa\u00e7\u00e3o, nenhuma entidade externa pode \"tirar a CPU \u00e0 for\u00e7a\" do processo. Esses escalonadores s\u00e3o f\u00e1ceis de implementar, mas, em contrapartida, permitem o abuso no tempo de CPU de um determinado programa, o que viola os crit\u00e9rios de um bom escalonador.</p>"},{"location":"resumos/mod2/#escalonadores-preemptivos","title":"Escalonadores Preemptivos","text":"<p>Nesse modelo de escalonamento, cada processo possui um tempo (time-slice) de posse do processador, e quando o tempo se esgota o SO retira o processador desse processo e permite que outro processo execute. Esse controle de tempo de execu\u00e7\u00e3o \u00e9 feito por interrup\u00e7\u00f5es.</p> <p>Os processadores modernos possuem um clock que gera as interrup\u00e7\u00f5es a uma frequ\u00eancia determinada, e o SO mant\u00e9m um contador (indicando o tempo m\u00e1ximo de perman\u00eancia do processo com a CPU) que \u00e9 decrementado a cada clock tick, se o contador chegar a zero, o tempo de perman\u00eancia do processo ter\u00e1 se esgotado.</p> <p>Esses escalonadores asseguram um uso mais balanceado da CPU e s\u00e3o utilizados na maioria dos SO modernos, mas geram complica\u00e7\u00f5es na programa\u00e7\u00e3o de processos concorrentes.</p>"},{"location":"resumos/mod2/#criterios-de-escalonamento","title":"Crit\u00e9rios de Escalonamento","text":"<p>Um bom escalonador deve estar atento aos crit\u00e9rios a seguir, embora seja imposs\u00edvel atingir todos:</p> <ul> <li>Justi\u00e7a: todos os processos devem ter chances justas de uso do processador (n\u00e3o s\u00e3o chances iguais);</li> <li>Efici\u00eancia: o processador deve estar ocupado sempre que houver trabalho a fazer;</li> <li>Minimizar o tempo de resposta: reduzir o tempo entre a entrada de usu\u00e1rio e a resposta dada;</li> <li>Minimizar o turnaround: reduzir o tempo desde o lan\u00e7amento do processo at\u00e9 seu t\u00e9rmino (soma de tempo de espera por recursos e tempo de utiliza\u00e7\u00e3o da CPU);</li> <li>Minimizar o waiting time: minimizar o tempo de espera pela CPU;</li> <li>Maximizar o throughput: maximizar o n\u00famero de tarefas em uma unidade de tempo;</li> </ul>"},{"location":"resumos/mod2/#algoritmos-de-escalonamento","title":"Algoritmos de Escalonamento","text":"<p>Algoritmos de Escalonamento visam dividir a utiliza\u00e7\u00e3o do processo entre processos que devem ser executados. Alguns dos algoritmos cl\u00e1ssicos s\u00e3o: First Come First Served, Round-Robin, Prioridades e Shortest Job First.</p>"},{"location":"resumos/mod2/#first-come-first-served","title":"First Come First Served","text":""},{"location":"resumos/mod2/#round-robin","title":"Round-Robin","text":"<p>Esse algoritmo permite que cada processo utilize o processador por um intervalo de tempo pr\u00e9-definido, denominado quantum. Quando o quantum se esgota, o processador \u00e9 dado a outro processo, em ordem de chegada.</p> round-robin.c<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n/*\nEntrada:\nA primeira linha da entrada \u00e9 um n\u00famero inteiro N, entre 1 e 100, indicando o n\u00famero de processos que ser\u00e3o escalonados.\nA segunda linha cont\u00e9m a janela de tempo (100 &lt; T &lt;= 1000) em MILISSEGUNDOS. As pr\u00f3ximas N linhas da entrada\npossuem um identificador \u00fanico (pid) e o tempo total de execu\u00e7\u00e3o em SEGUNDOS que esse processo precisa para executar.\nSa\u00edda:\nA sa\u00edda cont\u00e9m N linhas, onde os processos s\u00e3o impressos na ordem que terminaram a execu\u00e7\u00e3o. Considere que todos\nos processos iniciam a execu\u00e7\u00e3o no tempo 0. Para cada um dos processos, tamb\u00e9m \u00e9 impresso o tempo total quando o\nprocesso terminou a execu\u00e7\u00e3o, ou seja, o turnaround de cada processo.\nExemplo:\nExemplo de entrada\n3\n500\n23 6\n186 2\n59 2\nSa\u00edda para o exemplo de entrada\n186 (5500)\n59 (6000)\n23 (10000\n*/\ntypedef struct process\n{\nlong int pid;\nlong int remaining_time;\n} process;\nvoid remove_index(process *p, int start, int size)\n{\nfor (int i = start; i &lt; size; i++)\np[i] = p[i + 1];\n}\nvoid print_process(process p)\n{\nprintf(\"%ld(-%ld) \", p.pid, p.remaining_time);\n}\nvoid print_processes(process *p, int size)\n{\nfor (int i = 0; i &lt; size; i++)\nprint_process(p[i]);\nprintf(\"\\n\");\n}\nint main()\n{\nint qt_processes;\nlong int quantum;\nprocess *p;\n// Reading input\nscanf(\"%d\", &amp;qt_processes);\nscanf(\"%ld\", &amp;quantum);\n// Allocating memory for process array.\np = malloc(qt_processes * sizeof(process));\n// Reading processes\nlong int pid;\nlong int remaining_time;\nfor (int i = 0; i &lt; qt_processes; i++)\n{\nscanf(\"%ld %ld\", &amp;pid, &amp;remaining_time);\n// Converting from seconds to milisseconds\nremaining_time *= 1000;\np[i].pid = pid;\np[i].remaining_time = remaining_time;\n}\nlong int time = 0;\nwhile (qt_processes &gt; 0)\n{\nfor (int i = 0; i &lt; qt_processes; i++)\n{\nif (p[i].remaining_time &gt; quantum)\n{\ntime += quantum;\np[i].remaining_time -= quantum;\n}\nelse\n{\ntime += p[i].remaining_time;\nprintf(\"%ld (%ld)\\n\", p[i].pid, time);\nremove_index(p, i, qt_processes);\nqt_processes--;\ni -= 1;\n}\n// print_processes(p, qt_processes);\n}\n}\nreturn 0;\n}\n</code></pre>"},{"location":"resumos/mod2/#prioridades","title":"Prioridades","text":""},{"location":"resumos/mod2/#shortest-job-first","title":"Shortest Job First","text":""},{"location":"resumos/mod2/#referencias","title":"Refer\u00eancias","text":"<p>[1] TANENBAUM, A. S. Sistemas. Operacionais Modernos. 4\u00aa ed. Prentice Hall, 2016.</p>"},{"location":"resumos/mod3/","title":"M\u00f3dulo 3","text":""},{"location":"resumos/mod3/#referencias","title":"Refer\u00eancias","text":"<p>[1] TANENBAUM, A. S. Sistemas. Operacionais Modernos. 4\u00aa ed. Prentice Hall, 2016.</p>"},{"location":"resumos/mod4/","title":"Comunica\u00e7\u00e3o Entre Threads","text":""},{"location":"resumos/mod4/#referencias","title":"Refer\u00eancias","text":"<p>[1] TANENBAUM, A. S. Sistemas. Operacionais Modernos. 4\u00aa ed. Prentice Hall, 2016.</p>"},{"location":"resumos/mod5/","title":"M\u00f3dulo 5 - Problemas Cl\u00e1ssicos de IPC e Deadlock","text":""},{"location":"resumos/mod5/#relembrando","title":"Relembrando","text":"<p>Processos podem se comunicar utilizando mecanismos de IPC (Inter-process Communication), como mem\u00f3ria compartilhada ou comunica\u00e7\u00e3o por mensagens. A prote\u00e7\u00e3o que deve ser fornecida \u00e0 essa comunica\u00e7\u00e3o pode gerar alguns problemas, como por exemplo deadlock, que ocorre quando dois ou mais processos aguardam mutuamente por um recurso utilizado por um deles, e starvation, que ocorre quando os mecanismos de sincroniza\u00e7\u00e3o n\u00e3o permitem que o programa avance. Para exemplificar o problema do deadlock a literatura fornece alguns problemas cl\u00e1ssicos, como o do Jantar dos Fil\u00f3sofos.</p>"},{"location":"resumos/mod5/#problema-do-jantar-dos-filosofos","title":"Problema do Jantar dos Fil\u00f3sofos","text":"<p>Esse problema foi formulado e resolvido por Dijkstra em 1965 e consiste na seguinte situa\u00e7\u00e3o:</p> <p>Cinco fil\u00f3sofos est\u00e3o sentados em torno de uma mesa circular, cada qual com um prato de espaguete escorregadio, que para ser consumido necessita de dois garfos, mas n\u00e3o h\u00e1 garfos suficientes para que todos os fil\u00f3sofos comam ao mesmo tempo, j\u00e1 que h\u00e1 um garfo entre cada par de pratos (Figura 1).</p> <p> <p></p> <p>Figura 1. Ilustra\u00e7\u00e3o do problema do jantar dos fil\u00f3sofos. (Fonte: [1]). </p> <p>Portanto, cada fil\u00f3sofo compartilha seus garfos com os fil\u00f3sofos imediatamente \u00e0 esquerda e \u00e0 direita. A vida de um fil\u00f3sofo consiste em alternar per\u00edodos de pensar e se alimentar, sendo que depois de pensar durante muito tempo ele tentar\u00e1 se alimentar.</p> <p>Para se alimentar, o fil\u00f3sofo tenta pegar um garfo \u00e0 sua esquerda e \u00e0 sua direita, em qualquer ordem. Quando conseguir pegar os dois garfos, ele se alimentar\u00e1, e em seguida devolver\u00e1 os garfos \u00e0 mesa.</p> filosofos.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\nint qtPhilosophers = 5; // Number of philosophers.\nvoid think(int i)\n{\nprintf(\"Philosopher %d is THINKING...\\n\\n\", i);\nsleep(1);\n}\nvoid take_fork(int i)\n{\nprintf(\"Attempting to get fork at position %d.\\n\\n\", i);\n}\nvoid eat()\n{\nprintf(\"Eating...\\n\\n\");\nsleep(1);\n}\nvoid put_fork(int i)\n{\nprintf(\"Returning fork at position %d.\\n\\n\", i);\n}\nvoid philosopher(int i)\n{\nwhile (1)\n{\nint left = i;\nint right = (i + 1) % qtPhilosophers;\nthink(i);\ntake_fork(left);\ntake_fork(right);\neat();\nput_fork(right); // Returning in inverse order of retrieval\nput_fork(left);  // increases parallelism.\n}\n}\n</code></pre> <p>O problema acontece se todos os fil\u00f3sofos tentarem pegar o garfo ao mesmo tempo, a depender do escalonamento. Se o escalonamento fizer a troca de contexto ap\u00f3s a obten\u00e7\u00e3o do primeiro garfo, quando o N-\u00e9simo fil\u00f3sofo tentar pegar o garfo direito, este j\u00e1 ter\u00e1 sido obtido pelo fil\u00f3sofo \u00e0 sua direita.Ent\u00e3o, todos os fil\u00f3sofos ficar\u00e3o bloqueados tentando obter o outro garfo, e esse problema \u00e9 conhecido como deadlock (ou impasse).</p> <p>E se o fil\u00f3sofo devolver o garfo esquerdo se n\u00e3o conseguir obter o direito?</p> <p>Essa solu\u00e7\u00e3o pode minimizar o problema, mas ainda \u00e9 poss\u00edvel obter um escalonamento problem\u00e1tico, como no caso em que a troca de contexto ocorre ap\u00f3s um fil\u00f3sofo obter ou liberar um garfo, ter\u00edamos a situa\u00e7\u00e3o:</p> <pre><code>F1 obt\u00e9m o garfo, ..., F5 obt\u00e9m o garfo.\n\n&lt;Troca de contexto&gt;\n\nF1 testa e libera o garfo, ..., F5 testa e libera o garfo. \n\n&lt;Troca de contexto&gt;\n\nF1 obt\u00e9m o garfo, ..., F5 obt\u00e9m o garfo.\n</code></pre> <p>Essa situa\u00e7\u00e3o gera outro problema, embora os fil\u00f3sofos estejam processando, n\u00e3o h\u00e1 nenhum tipo de progresso. Tal problema \u00e9 chamado de starvation (inani\u00e7\u00e3o).</p> <p>Uma terceira solu\u00e7\u00e3o seria o fil\u00f3sofo esperar um tempo aleat\u00f3rio para tentar obter o garfo novamente, caso n\u00e3o consiga na primeira tentativa, que \u00e9 uma solu\u00e7\u00e3o muito boa do ponto de vista pr\u00e1tico, inclusive utilizada no tr\u00e1fego de pacotes em cabos de rede, mas n\u00e3o \u00e9 uma solu\u00e7\u00e3o completa do ponto de vista te\u00f3rico, j\u00e1 que existe uma m\u00ednima probabilidade de que dois fil\u00f3sofos esperem a mesma quantidade de tempo, ficando bloqueados.</p> <p>\u00c9 poss\u00edvel resolver o problema utilizando sem\u00e1foros bin\u00e1rios, protegendo uma se\u00e7\u00e3o cr\u00edtica com o uso de um mutex. Antes de come\u00e7ar a pegar os garfos, um fil\u00f3sofo realiza down em um mutex e ap\u00f3s substituir os garfos ele realiza up nesse mutex. Essa solu\u00e7\u00e3o, embora correta do ponto de vista te\u00f3rico, apresenta um problema de desempenho, pois embora os fil\u00f3sofos possam pensar em paralelo, apenas um pode se alimentar por vez, embora seja poss\u00edvel que mais do que um se alimente em paralelo.</p> <p>A melhor solu\u00e7\u00e3o, do ponto de vista te\u00f3rico, \u00e9 proteger as fun\u00e7\u00f5es de pegar e liberar os garfos com um sem\u00e1foro bin\u00e1rio e um array de sem\u00e1foros (<code>s</code>), no qual cada fil\u00f3sofo <code>i</code> incrementa <code>s[i]</code> caso obtenha os dois garfos, ou bloqueia nele esperando o vizinho liberar o garfo.</p> jantar_dos_filosofos.c<pre><code>#include &lt;semaphore.h&gt;\n#include &lt;pthread.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n#define N 5\n#define LEFT (i + N - 1) % N\n#define RIGHT (i + 1) % N\n#define THINKING 0\n#define HUNGRY 1\n#define EATING 2\n// Sem\u00e1foro bin\u00e1rio para proteger as fun\u00e7\u00f5es de pegar e guardar os garfos.\nsem_t mutex;\n// Estados dos fil\u00f3sofos, entre {HUNGRY, EATING, THINKING}.\nint state[N];\n// Array de sem\u00e1foros para os N fil\u00f3sofos.\nsem_t s[N];\n// N\u00famero dos fil\u00f3sofos definido globalmente para ser\n// acessado pelos processos filhos.\nint phil[N] = {0, 1, 2, 3, 4};\n// Testa se os garfos est\u00e3o livres para o fil\u00f3sofo i,\n// o que ocorre se os seus vizinhos n\u00e3o estiverem comendo.\nvoid test(int i)\n{\nif (\nstate[i] == HUNGRY &amp;&amp; state[LEFT] != EATING &amp;&amp; state[RIGHT] != EATING)\n{\nstate[i] = EATING;\nprintf(\"Philosopher %d takes forks %d and %d. \\n\", i, LEFT, RIGHT);\nprintf(\"Philosopher %d is EATING.\\n\", i);\nsem_post(&amp;s[i]);\n}\n}\n// Fun\u00e7\u00e3o \"at\u00f4mica\" para pegar ambos garfos, protegida pelo mutex.\nvoid take_forks(int i)\n{\nsem_wait(&amp;mutex);\nstate[i] = HUNGRY;\nprintf(\"Philosopher %d is HUNGRY.\\n\", i);\ntest(i);\nsem_post(&amp;mutex);\nsem_wait(&amp;s[i]);\nsleep(1);\n}\n// Fun\u00e7\u00e3o \"at\u00f4mica\" para devolver os garfos e \n// sinalizar os vizinhos.\nvoid put_forks(int i)\n{\nsem_wait(&amp;mutex);\nstate[i] = THINKING;\nprintf(\"Philosopher %d is putting down forks %d and %d.\\n\", i, LEFT, RIGHT);\nprintf(\"Philosopher %d is THINKING.\\n\", i);\ntest(LEFT);\ntest(RIGHT);\nsem_post(&amp;mutex);\n}\nvoid think(int i)\n{\nprintf(\"Philosopher %d is THINKING...\\n\", i);\nsleep(1);\n}\nvoid *philosopher_semaphores(void *num)\n{\nwhile (1)\n{\nint *i = num;\nthink(*i);\ntake_forks(*i);\nsleep(1);\nput_forks(*i);\n}\n}\n// Aplica\u00e7\u00e3o utilizando pthreads.\nvoid main()\n{\nint i;\npthread_t thread_id[N];\nsem_init(&amp;mutex, 0, 1);\nfor (i = 0; i &lt; N; i++)\nsem_init(&amp;s[i], 0, 0);\nfor (i = 0; i &lt; N; i++)\npthread_create(&amp;thread_id[i], NULL, philosopher_semaphores, &amp;phil[i]);\nfor (i = 0; i &lt; N; i++)\npthread_join(thread_id[i], NULL);\n}\n</code></pre> <p>Outras solu\u00e7\u00f5es completas para o jantar dos fil\u00f3sofos s\u00e3o:</p> <ul> <li>Permitir que apenas quatro fil\u00f3sofos sentem-se \u00e0 mesa;</li> <li>Permitir que o fil\u00f3sofo pegue o garfo da esquerda apenas se o da direita estiver livre (uma opera\u00e7\u00e3o at\u00f4mica para pegar e verificar);</li> <li>Permitir que um fil\u00f3sofo \u00edmpar pegue primeiro o seu garfo da esquerda e depois o da direita, enquanto um par pega o da direita e depois o da esquerda.</li> </ul>"},{"location":"resumos/mod5/#recursos","title":"Recursos","text":"<p>Recursos s\u00e3o objetos que um processo pode adquirir de maneira exclusiva, quando apenas um processo pode utilizar por vez (exclus\u00e3o m\u00fatua), ou n\u00e3o. Um recurso \u00e9 qualquer coisa que pode ser adquirida, usada e liberada, e pode ser classificado em preempt\u00edvel ou n\u00e3o preempt\u00edvel.</p> <p>Recursos preempt\u00edveis: podem ser retirados do propriet\u00e1rio por uma entidade externa sem causar-lhe preju\u00edzo, como por exemplo mem\u00f3ria n\u00e3o utilizada.</p> <p>Recursos n\u00e3o preempt\u00edveis: n\u00e3o podem ser tomados \u00e0 for\u00e7a, o processo que est\u00e1 utilizando deve liber\u00e1-lo espontaneamente, como por exemplo uma impressora.</p> <p>Em geral, deadlocks ocorrem em recursos n\u00e3o preempt\u00edveis, j\u00e1 que nos preempt\u00edveis uma simples transfer\u00eancia de recursos resolveria o problema.</p> <p>Para utilizar um recurso, um processo deve solicit\u00e1-lo, us\u00e1-lo e liber\u00e1-lo. Se o recurso n\u00e3o estiver dispon\u00edvel quando for solicitado, o processo \u00e9 for\u00e7ado a esperar.</p>"},{"location":"resumos/mod5/#deadlock","title":"Deadlock","text":"<p>O deadlock \u00e9 definido, formalmente, da seguinte maneira:</p> <p>\u201cUm conjunto de processos estar\u00e1 em situa\u00e7\u00e3o de deadlock se cada processo no conjunto estiver esperando por um evento que apenas outro processo no conjunto pode causar.\u201d</p>"},{"location":"resumos/mod5/#modelagem-de-deadlock","title":"Modelagem de Deadlock","text":""},{"location":"resumos/mod5/#referencias","title":"Refer\u00eancias","text":"<p>[1] TANENBAUM, A. S. Sistemas. Operacionais Modernos. 4\u00aa ed. Prentice Hall, 2016.</p>"},{"location":"summaries/ch1/","title":"Introdu\u00e7\u00e3o","text":"<p>Linux</p> <ul> <li>O linux \u00e9 apenas o kernel, em torno do qual orbitam as distribui\u00e7\u00f5es.</li> <li>\u00c9 um sistema veloz e possui comunidade ampla e atuante.</li> <li>Seguran\u00e7a satisfat\u00f3ria.</li> </ul> <p>MINIX</p> <ul> <li>SO criado pelo Tanenbaum com fins did\u00e1ticos.</li> <li> <p>O Linux foi desenvolvido com base no MINIX. A primeira compila\u00e7\u00e3o do Linux foi em um MINIX do Linus Torvalds.</p> </li> <li> <p>Poucas op\u00e7\u00f5es de plataformas: x86 e arm.</p> </li> </ul> <p>FreeBSD</p> <ul> <li>N\u00e3o \u00e9 apenas um kernel, mas um sistema completo.</li> <li>UNIX.</li> <li>Derivado do Linux.</li> <li>Poucas op\u00e7\u00f5es de plataformas: x86, amd64 e arm.</li> </ul> <p>OpenBSD</p> <ul> <li>Foco em seguran\u00e7a em corretude.</li> <li>Red Team Field Manual.</li> <li>Provedor de ferramentas importantes para o mundo *nix: OpenSSH, OpenNTPD, OpenSMPD, realyd, spamd, httpd, tmux etc.</li> <li> <p>Suporte a ampla gama de arquiteturas.  NetBSD</p> </li> <li> <p>Amplo suporte a hardware.</p> </li> <li>Prop\u00f4s gerenciador de pacotes (pckgsrc) que serviu de inspira\u00e7\u00e3o aos gerenciadores de pacotes dos outros BSDs e Linux.</li> <li>Uso de linguagens n\u00e3o usuais no kernel (LUA).</li> </ul>"},{"location":"summaries/ch1/#o-que-e-um-so","title":"O que \u00e9 um SO?","text":"<p>Um sistema operacional \u00e9 um programa que gerencia o hardware de um computador. \u00c9 um intermedi\u00e1rio entre o usu\u00e1rio de um computador e o hardware deste.</p> <p>Os principais objetivos de um sistema operacional s\u00e3o:</p> <ul> <li>Executar programas do usu\u00e1rio e facilitar a resolu\u00e7\u00e3o de problemas do usu\u00e1rio.</li> <li>Tornar o uso de um sistema computacional conveniente para o usu\u00e1rio.</li> <li>Usar o hardware de um computador de maneira eficiente.</li> </ul> <p>Como um sistema operacional \u00e9 extenso e complexo, ele deve ser criado parte por parte. Cada uma dessas partes deve ser uma por\u00e7\u00e3o bem delimitada do sistema, com entradas, sa\u00eddas e fun\u00e7\u00f5es cuidadosamente definidas.</p>"},{"location":"summaries/ch1/#o-que-sos-fazem","title":"O que SOs fazem","text":"<p>Um sistema computacional pode ser dividido em quatro componentes:</p> <ul> <li>Hardware: prov\u00ea recursos computacionais b\u00e1sicos.</li> <li>CPU, mem\u00f3ria, dispositivos I/O.</li> <li>Sistema Operacional</li> <li>Controla e coordena o uso do hardware por diversas aplica\u00e7\u00f5es e usu\u00e1rios.</li> <li>Aplica\u00e7\u00f5es: define a forma como recursos computacionais s\u00e3o usados para resolver problemas computacionais do usu\u00e1rio.</li> <li>Editores de texto, compiladores, navegadores de internet, sistemas de banco de dados, jogos.</li> <li>Usu\u00e1rios</li> <li>Pessoas, m\u00e1quinas ou outros computadores.</li> </ul> <p> </p> <p>  Figure: Componentes do sistema computacional. Source: [1] </p> <p>Podemos explorar os sistemas operacionais de dois pontos de vista:</p> <ul> <li>Ponto de vista do usu\u00e1rio</li> </ul> <p>Para a maioria dos usu\u00e1rios um sistema operacional \u00e9 projetado para um \u00fanico usu\u00e1rio usufruir dos recursos, tal sistema tem o objetivo de facilitar o uso, com algum cuidado em rela\u00e7\u00e3o \u00e0 performance e nenhuma ao uso de recursos.</p> <p>Em outros casos, o usu\u00e1rio utiliza um terminal conectado a um mainframe ou minicomputador, acessado tamb\u00e9m por outros usu\u00e1rios por meio de outros terminais. Nesse caso, o sistema operacional \u00e9 projetado para maximizar a utiliza\u00e7\u00e3o de recursos, para garantir que CPU, mem\u00f3ria e I/O s\u00e3o utilizados de forma eficiente e equilibrada entre os usu\u00e1rios.</p> <p>H\u00e1 tamb\u00e9m os usu\u00e1rios de workstations conectadas \u00e0 redes de workstations e servidores. Esses usu\u00e1rios t\u00eam recursos pr\u00f3prios, por\u00e9m partilham recursos de rede e servidores. Nesse caso, o sistema operacional \u00e9 projetado para encontrar um meio termo entre usabilidade pessoal e uso dos recursos.</p> <ul> <li>Ponto de vista do sistema</li> </ul> <p>Do ponto de vista do computador, o SO \u00e9 o programa mais intimamente ligado ao hardware. Podemos ver um sistema operacional como um alocador de recursos, agindo como gerenciador desses recursos e resolvendo os conflitos de acesso a eles.</p> <p>Um sistema operacional tamb\u00e9m pode ser visto como um programa de controle, isto \u00e9, um programa que controla a execu\u00e7\u00e3o dos programas para prevenir erros e uso impr\u00f3prio do computador. Ele preocupa-se, especialmente, com a opera\u00e7\u00e3o e controle dos dispositivos I/O.</p> <p>Defini\u00e7\u00e3o de Sistema Operacional</p> <p>A defini\u00e7\u00e3o mais comum \u00e9: o sistema operacional \u00e9 o programa sendo executado durante todo o funcionado no computador, geralmente chamado de kernel.</p>"},{"location":"summaries/ch1/#sistemas-computacionais","title":"Sistemas computacionais","text":""},{"location":"summaries/ch1/#organizacao","title":"Organiza\u00e7\u00e3o","text":"<p>Um computador moderno de prop\u00f3sito geral consiste de uma ou mais CPUs e uma quantidade de controladores de dispositivos conectados por meio de um barramento comum, que prov\u00ea acesso \u00e0 mem\u00f3ria compartilhada. A CPU e os dispositivos podem funcionar em paralelo, competindo por ciclos de mem\u00f3ria, e para assegurar o acesso ordenado \u00e0 mem\u00f3ria, o controlador da mem\u00f3ria sincroniza o acesso \u00e0 ela.</p> <p> </p> <p>  Figure: Organiza\u00e7\u00e3o de um sistema computacional moderno. Source: [1] </p> <p>Para que o computador comece a funcionar, ap\u00f3s ser ligado ou reiniciado, \u00e9 necess\u00e1rio que ele tenha um programa inicial para rodar, chamado de bootstrap program, que tende a ser simples. Geralmente tal programa \u00e9 armazenado no hardware do computador na mem\u00f3ria de leitura (ROM) ou na mem\u00f3ria program\u00e1vel e apag\u00e1vel el\u00e9trica (EEPROM) e \u00e9 conhecido por firmware. Ele inicializa todos os aspectos do sistema, desde os registradores da CPU, at\u00e9 os controladores de dispositivos e conte\u00fado da mem\u00f3ria. O programa de bootstrap deve saber como carregar o sistema operacional e execut\u00e1-lo, localizando o kernel do sistema operacional e carregando-o na mem\u00f3ria.</p> <p>Ap\u00f3s o carregamento do kernel, ele pode come\u00e7ar a fornecer servi\u00e7os para o sistema e seus usu\u00e1rios. Alguns servi\u00e7os s\u00e3o fornecidos fora do kernel, por programas de sistema que s\u00e3o carregados na mem\u00f3ria em tempo de inicializa\u00e7\u00e3o e tornam-se processos do sistema, ou daemons do sistema, que rodam durante todo o tempo de execu\u00e7\u00e3o do kernel.</p> <p>A ocorr\u00eancia de um evento geralmente \u00e9 sinalizada por meio de uma interrup\u00e7\u00e3o, tanto do hardware quanto do software. O hardware pode provocar uma interrup\u00e7\u00e3o a qualquer instante enviando um sinal para a CPU, geralmente por meio do barramento. O software pode provocar uma interrup\u00e7\u00e3o executando uma chamada de sistema (system call ou monitor call).</p> <p>Quando a CPU \u00e9 interrompida, ela para imediatamente o que estiver executando e transfere a execu\u00e7\u00e3o para uma localiza\u00e7\u00e3o fixada, que geralmente cont\u00e9m o endere\u00e7o de in\u00edcio da rotina a ser executada. A rotina de interrup\u00e7\u00e3o \u00e9 executada e, ap\u00f3s sua finaliza\u00e7\u00e3o, a CPU continua a computa\u00e7\u00e3o que foi interrompida.</p> <p> </p> <p>  Figure: Linha do tempo de interrup\u00e7\u00f5es. Source: [1] </p> <p>Como existe uma quantidade predeterminada de interrup\u00e7\u00f5es poss\u00edveis, uma tabela de ponteiros para rotinas de interrup\u00e7\u00e3o pode ser usada para aumentar a velocidade de execu\u00e7\u00e3o. Geralmente essa tabela \u00e9 armazenada em endere\u00e7os baixos de mem\u00f3ria, onde s\u00e3o guardados os endere\u00e7os para as rotinas de interrup\u00e7\u00e3o de diversos dispositivos. Esse array, ou vetor de interrup\u00e7\u00f5es, \u00e9 indexado por um n\u00famero \u00fanico por dispositivo, fornecendo o endere\u00e7o da rotina a ser executada.</p> <p>O endere\u00e7o de retorno ap\u00f3s a interrup\u00e7\u00e3o tamb\u00e9m deve ser armazenado, geralmente na pilha do sistema, e \u00e9 recuperado para o contador de programa ap\u00f3s a interrup\u00e7\u00e3o.</p>"},{"location":"summaries/ch1/#computer-system-operation","title":"Computer-System Operation","text":"<ul> <li>I/O devices and the CPU can execute concurrently</li> <li>Each device controller is in charge of a particular device type and has a local buffer</li> <li>CPU moves data from/to main memory to/from local buffers</li> <li>I/O goes from the device to the controller's buffer</li> <li>Device controller informs CPU that it's finished its operation by causing and interrupt</li> </ul>"},{"location":"summaries/ch1/#common-functions-of-interrupts","title":"Common Functions of Interrupts","text":"<ul> <li>The interrupt vector contains the addresses of all the service routines.</li> <li>interrupt architecture must save the address of the interrupted instruction</li> <li>A trap or exception is a software-generated interrupt caused either by and error or user request</li> <li>An operating system is interrupt driven</li> </ul>"},{"location":"summaries/ch1/#interrupt-handling","title":"Interrupt Handling","text":"<ul> <li>The operating system preserves the state of the CPU by storing registers and the program counter</li> <li>Determines which type of interrupt has occurred:</li> <li>polling</li> <li>vectored interrupt system</li> <li>Separate segments of code determine what action should be taken for each type of interrupt</li> </ul>"},{"location":"summaries/ch1/#io-structure","title":"I/O Structure","text":""},{"location":"summaries/ch1/#referencia-bibliografica","title":"Refer\u00eancia Bibliogr\u00e1fica","text":"<ul> <li>SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition.</li> </ul>"},{"location":"summaries/ch2/","title":"Operating Systems","text":""},{"location":"summaries/ch2/#operating-system-services","title":"Operating system services","text":"<ul> <li>OS provide an environment for execution of programs and services to programs and users.</li> <li>The specif services vary among different OS, but we can identify common classes, as figure 1 shows.</li> </ul>  Figure 1: Services provided by OS. Source: [1]  <p>Set of operating-system services that are helpful do the user:</p> <ul> <li>User interface: Almost all OS have a user interface (UI)</li> <li>Varies between CLI, GUI and Batch</li> <li>Program execution:The system must be able to load a program into memory and to run that program, and execute it, either normally or abnormally (indicating error)</li> <li>I/O operations: Users usually cannot control I/O devices directly, therefore, the OS must provide a means to do I/O.</li> <li>File-system manipulation: Programs need to read and write files and directories, some OS also include permissions management to allow or deny access to files or directories based on file ownership</li> <li>Communications: Communication may occur between processes that are executing on the same computer or between processes that are execution on different computer systems tied together by a computer network. The communication may be implemented via shared memory or message passing</li> <li>Error detection: the OS needs to be detecting and correcting erros constantly. Erros may occur in the CPU and memory hardware, in I/O devices and the user program</li> </ul> <p>Some OS functions exists not for helping the user, but rather for ensuring the efficient operation of the system itself:</p> <ul> <li>Resource allocation: the OS manages many different types of resources, such as CPU cycles, main memory, I/O devices and file storage</li> <li>Accounting: we want to keep track of which users use how much and what kinds of computer resources</li> <li>Protection and security: the owners of information stored in a multiuser or networked computer system may want ot control use of that information. When several separate processes execute concurrently, it shouldn't be possible for one process to interfere with the others or the OS itself.</li> <li>Protection involves ensuring that all access to system resources is controlled</li> <li>Security of the system from outsiders require user authentication, extends to defending external I/O devices from invalid access attempts</li> </ul>"},{"location":"summaries/ch2/#user-and-operating-system-interface","title":"User and Operating-System Interface","text":""},{"location":"summaries/ch2/#command-interpreters-cli","title":"Command Interpreters (CLI)","text":"<ul> <li>Allows direct command entry, fetches a command from user and executes it.</li> <li>Some OS include the command interpreter in the kernel, others, like Windows and UNIX, treats the command interpreter as a special program that's running when a job's initiated or when a user first logs on.</li> <li>On system with multiple command interpreters to choose from, the interpreters are known as shells.</li> <li>Its main function is to get and execute user-specified commands, like create, delete, list, etc.</li> </ul> <p>The commands can be implemented in two general ways:</p> <ul> <li>The command interpreter itself contains the code to execute the command, which means the number of commands determines the size of the command interpreter;</li> <li>The commands are implemented through system programs, in this case the command interpreter doesn't understand the command, only uses its name to search for a corresponding file to load into memory and execute.</li> </ul> <p>For example, the command above will search for a file called 'rm', load it into memory and execute with the parameter 'file.txt'</p> <pre><code>  rm file.txt\n</code></pre> <p>This second approach enables the user to add custom commands without the need to alter the command interpreter, and it's used by UNIX, among other operating systems.</p>"},{"location":"summaries/ch2/#graphical-user-interfaces-gui","title":"Graphical User Interfaces (GUI)","text":"<ul> <li>User friendly desktop metaphor interface</li> <li>Relies its use in mouse, keyboard, monitor and touchscreen;</li> <li>Contains icons, folders, pointers, etc;</li> <li>Invented at Xerox PARC, but became more widespread with the advent of Apple Macintosh computers in the 80s;</li> <li>Most systems now include both CLI and GUI interfaces</li> <li>MS Windows is GUI with the CLI \"command\" shell;</li> <li>Apple Mac OS X is \"Aqua\" GUI interface with UNIX kernel underneath and shells available;</li> <li>Unix and linux have CLI with optional GUI interfaces (CDE, KDE, GNOME);</li> </ul>"},{"location":"summaries/ch2/#system-calls","title":"System Calls","text":"<ul> <li>Provides an interface to the services made available by an operating system.</li> <li>Typically written in a high-level language (C or C++).</li> <li>Mostly accessed by programs via a high-level API rather than direct system call use.</li> <li>The most common APIs are Win32 API for Windows, POSIX API for POSIX-based system (all versions of UNIX, Linux and Mac OS X) and Java API for the JVM.</li> <li>A programmer accesses the API via a library of code provided by the operating system.</li> <li>In UNIX and Linux, the library for C programs is libc.</li> <li>The use of APIs offers some benefits, among which is the potability of programs between Operational Systems.</li> <li>For most programming languages, the run-time support system provides a system call interface that serves as a the link to system calls made available by the OS, as illustrated by Figure 2 below.</li> </ul>  Figure 2: Handling of the open() system call. Source: [1]  <ul> <li>Typically, a number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers.</li> <li>The system call interface invokes the intended system call in OS kernel and return status of the system call and any return values.</li> <li>The caller need to know nothing about how the system call is implemented, just need to obey the API and understand what OS will do as a result call. Most details of OS interface is hidden from programmer by API.</li> </ul> <p>There are three general methods used to pass parameters to the OS:</p> <ul> <li>The simplest way, passing the parameters in registers.</li> <li>Parameters stored in a block or table in memory, and address of block passed as a parameter in a register (approach taken by Linux and Solaris), as illustrated by Figure 3.</li> <li>Parameters pushed onto the stack by the program and popped off the stack by the OS.</li> </ul> <p> </p>  Figure 3: System call parameter passing. Source: [1]"},{"location":"summaries/ch2/#types-of-system-calls","title":"Types of System Calls","text":"<p>System calls can be grouped in six categories, and the types of system calls for each one are represented in the Table 1 below.</p> Category Types of system calls Process control - end, abort  - load, execute  - create process, terminate process  - get process attributes, set process attributes  - wait for time  - wait event, signal event  - allocate and free memory  File management - create \ufb01le, delete \ufb01le  - open, close  - read, write, reposition  - get \ufb01le attributes, set \ufb01le attributes  Device management - request device, release device  - read, write, reposition  - get device attributes, set device attributes  - logically attach or detach devices  Information maintenance - get time or date, set time or date  - get system data, set system data  - get process, \ufb01le, or device attributes  - set process, \ufb01le, or device attributes  Communications - create, delete communication connection  - send, receive messages  - transfer status information  - attach or detach remote devices   Table 1: Types of system calls per category. Source: [1]  <p> </p>  Figure 4: Windows and UNIX system calls. Source: [1]  <p> </p>  Figure 5: Types of system calls per category. Source: [1]"},{"location":"summaries/ch2/#examples","title":"Examples","text":""},{"location":"summaries/ch2/#system-programs","title":"System Programs","text":"<ul> <li>System programs provide a convenient environment for program development and execution, they can be divided into:</li> <li>File manipulation</li> <li>Status information sometimes stored in a File modification</li> <li>Programming language support</li> <li>Program loading and execution</li> <li>Communications</li> <li>Background services</li> <li>Application programs</li> </ul>"},{"location":"summaries/ch2/#references","title":"References","text":"<p>[1] SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition.</p>"},{"location":"summaries/ch3/","title":"Processes","text":"<p>Early computers allowed only one program to be executed at a time, and it had complete control of the system and access to all system's resources. Nowadays computer systems allow multiple programs to be loaded into memory and executed simultaneously. These changes resulted in the notion of process, which is a program in execution. A system consists of a collection of processes, either operating system processes executing system code and user processes executing user code. All theses processes can execute concurrently with the CPU (or CPUs) multiplexed between them, which makes the computer more productive.</p>"},{"location":"summaries/ch3/#process-concept","title":"Process Concept","text":"<p>A batch system executes jobs, whereas a time-shared system has user programs or tasks. These activities are called processes.</p> <p>A process is a program in execution, and its more than the program code (text section), as it includes the current activity (program counter) and the content of the processor's registers. It also includes temporary data (stored in the stack), global variables (data section) and may include a dynamically allocated memory, the heap. This structure is illustrated in Figure 1.</p> <p> </p> <p>Figure 1: The process structure in memory. Source: [1]</p> <p>A program is simply a passive entity, often called a executable file. A process is a active entity, with a program counter specifying the next instruction to be executed and a set of resources. A program by itself isn't a process until its running, i. e., a program becomes a process when it's loaded into memory.</p> <p>During execution, a process changes its state, which is defined partly by its current activity. A process state may be one of the represented in Table 1, the state name can change among operating systems.</p> State Description New The process is being created Running Its instructions are being executed Waiting The process's waiting for some event, such as I/O completion Ready The process's waiting to be assigned to a processor Terminated The process has finished execution <p>Table 1: Process states. Source: [1]</p> <p> Only one process can be running at any processor at any time, but many processes may be ready and waiting. The Figure 2 represents the relationship between the different process states.</p> <p> </p> <p> Figure 2: Diagram of process state. Source: [1] </p> <p>Each process is represented in the operating system by a process control block (PCB), also called task control block, represented in the Figure 3. It contains information about the process state, the next instruction to be executed for this process (program counter), the state of the CPU registers (Figure 4), CPU-scheduling information about priority, scheduling parameters and memory management information, such as base and limit registers and page tables, accounting information and I/O status information.</p> <p> </p> <p> Figure 3: The PCB structure. Source: [1] </p> <p> </p> <p> Figure 4: CPU switch between processes. Source: [1] </p> <p> Most modern operating system allow a process to have multiple threads of execution and execute more than one task at a time. In a multicore system, multiple threads can run in parallel.</p>"},{"location":"summaries/ch3/#process-scheduling","title":"Process Scheduling","text":"<p>The objective of multiprogramming is having some process running at all times, to maximize CPU utilization. The objective of time sharing it to switch the CPU among processes so frequently that users can interact with each program while it's running. The process scheduler selects an available process to execute on the CPU, in a single processor system, there'll be only one process running at a time.</p> <p>When a process enter the system, it joins the job queue, those which are ready and waiting to execute are kept in a list called ready queue, which is usually a linked list containing the PCB of each process and a pointer to the next process in line. The device queue order processes waiting for a particular I/O, and each device has its own device queue (Figure 5).</p> <p> </p> <p> Figure 5: Diagram of queueing of processes. Source: [1] </p> <p>A process migrate between scheduling queues during its lifetime, and the operating system select process from these queues through the appropriate scheduler. Often, in a batch system, more processes are submitted that can be executed immediately, so these processes are spooled to a mass-storage decide and kept for later execution. The long-term scheduler (or job scheduler) selects processes from this pool and loads them into memory. The short-term scheduler (or CPU scheduler) select among the processes that are ready and allocate the CPU to one of them. The long-term scheduler controls the degree of multiprogramming, which is the number of processes in memory.</p> <p>In general, most processes are either I/O bound or CPU bound, the first kind spends more time doing I/O than it spends computing, and for the later the opposite is true. A better performance lays in the long-term scheduler balancing its choices between I/O bound and CPU bound processes.</p> <p>Some operating systems have an intermediate level of scheduling, the medium-term scheduler (Figure 6), that removes processes from memory and reinserts them after a while (swapping). Swapping may be necessary according to lack of resources.</p> <p> </p> <p> Figure 6: Addition of medium-term scheduler in the queueing diagram. Source: [1] </p> <p>When an interrupt occurs, the system needs to save the context of the process running on the CPU so it can be resumed later, and the information is stored in the PCB of the process. The operations of saving and restoring this data are called, respectively, state save and state restore. A context switch is the operation of switching CPU between processes, it requires saving the state of the current process and restoring the state of the next process to execute.</p>"},{"location":"summaries/ch3/#operations-on-processes","title":"Operations on Processes","text":""},{"location":"summaries/ch3/#process-creation","title":"Process Creation","text":"<p>During its execution, a (parent) process may create several child processes, which can also create child processes, creating a tree (Figure 7) of processes. In most operating systems, processes are identified by a unique process identifier (pid), usually an integer number.</p> <p> </p> <p> Figure 7: a tree of processes on a typical Linux system. Source: [1] </p> <ul> <li>The <code>init</code> process (pid = 1) serves as the root parent process for all user processes.</li> <li>The <code>kthreadd</code> process is responsible for creating additional processes that perform tasks on behalf of the kernel.</li> <li>The <code>sshd</code> process is responsible for managing clients that connect to the system by using ssh.</li> </ul> <p>On UNIX and Linux systems the processes can be listed using the <code>ps</code> command.</p> <pre><code>ps -el\n</code></pre> <p>When a process creates a child, it may have to share resources and data with it. Besides that, there are two possible scenarios of execution:</p> <ol> <li>The parent continues to execute concurrently with its children;</li> <li>The parent waits until some or all of its children have terminated;</li> </ol> <p>And two scenarios regarding address-space:</p> <ol> <li>The child process is a duplicate of the parent process (it has the same program and data as the parent);</li> <li>The child process has a new program loaded into it;</li> </ol> <p>The system call <code>fork()</code> creates a new process, which is a copy of the address space of the original process. Both processes, parent and child, continue executing after the <code>fork()</code>.</p> <pre><code>#include &lt;sys/types.h&gt;\n#include &lt;unistd.h&gt;\npid_t fork(void);\n</code></pre> <p>The return code of <code>fork()</code> is zero in the child process, and the PID of the child process in the parent. In case of error, -1 is returned in the parent and no child process is created, which is illustrated by the code snippet 1 and its output (Figure 8). After a fork system call, one of the two processes uses the <code>exec()</code> system call to replace the process's memory space with a new program.</p> <p> The <code>exec()</code> system call loads a binary file into memory (destroying the memory image of the program containing the exec system call) and starts its execution. The parent process can then create more children or simply <code>wait()</code> to move itself off the ready queue until the child terminates (Figure 9).</p> <pre><code>#include &lt;sys/types.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\nint main()\n{\npid_t pid;\npid = fork();\nif (pid &lt; 0)\n{\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0)\n{\nprintf(\"\\tExecuting child!\\n\");\nexeclp(\"/bin/ps\", \"-l\", NULL); // Executes the /bin/ps file with the arguments: \"-l NULL\"\n}\nelse\n{\nwait(NULL); // Waits for the termination of the child process.\nprintf(\"\\tChild PID was: %d and its now terminated.\\n\", pid);\nexeclp(\"/bin/ps\", \"-l\", NULL);\n}\nreturn 0;\n}\n</code></pre> <p> Code snippet 1: Demonstration of the fork system call. Source: [1] </p> <p> </p> <p> Figure 8: Output of the code snippet. Source: author. </p> <p> </p> <p> Figure 9: Process creation using the fork system call. Source: [1] </p>"},{"location":"summaries/ch3/#process-termination","title":"Process Termination","text":"<p>A process terminates when it finishes executing its final statement and asks the operating system to delete it by the exit() system call. The process may return a status value to its parent via the wait system call. All the resources of the process are deallocated by the OS. Processes can also terminate via an appropriated system call, which usually can only be invoked by its parent process.</p> <p>Some systems don't allow child processes to keep running after its parent has been terminated, this phenomenon is called cascading termination, and normally initiated by the operating system.</p> <p> In Linux and UNIX system a process can be terminated by the <code>exit()</code> system call, that can be invoked directly or by a <code>return</code> statement in <code>main()</code>.</p> <p>A parent can wait for the termination of a child process using the <code>wait()</code> system call. This system call can receive a parameter to obtain the exit status of the child and it returns the pid of the terminated child. When a process terminate its entry in the process table must remain there until the parent calls <code>wait()</code>, because the process table contains the process's exit status. During the period of time when a process is terminated but its parent hasn't yet called <code>wait()</code>, a process is in a zombie state.</p> <p>If a process terminated without invoking wait, it would leave its child processes as orphans. Linux and UNIX solve this problem assigning the <code>init</code> process as the new parent for orphan processes. The init process periodically invokes wait, collecting the exit status of the orphan processes and releasing them from the process table.</p>"},{"location":"summaries/ch3/#interprocess-communication","title":"Interprocess Communication","text":"<p>Processes executing concurrently may be either independent of cooperating processes. Independent processes cannot affect or be affected by the other processes executing, any process that doesn't share data with other processes is independent. A process is cooperating if it can affect or be affected by other processes, clearly processes that share data area cooperating processes. Cooperation can be useful for information sharing, computation speedup, modularity and convenience.</p> <p>Cooperating processes require an interprocess communication (IPC) mechanism, and there are two fundamental models of IPC: shared memory and message passing. In the first, a region of memory is shared between processes and they can share information by reading and writing in this region. In the later, communication takes place by means of messages exchanged between the cooperating processes. Most systems implement both models, message passing is preferred when smaller amounts of data are shared, but shared memory can be faster than message passing.</p> <p> </p> <p> Figure 10: Interprocess communication models: message passing (a) and shared memory (b). Source: [1] </p>"},{"location":"summaries/ch3/#shared-memory-systems","title":"Shared-Memory Systems","text":"<p>The shared-memory region usually resides in the address space of the process creating it. The processes that wish to communicate using this segment must attach it to their address space. Usually processes aren't allowed to access each others memory, so this model of communication requires that one or more processes agree to remove this restriction. The data then can be shared by writing and reading in the shared area, and the operating system doesn't control how the communication flows, so it's up to the processes assuring that there's no simultaneously writing to the same location.</p>"},{"location":"summaries/ch3/#message-passing-systems","title":"Message-Passing Systems","text":"<p>Message passing allow processes to communicate and synchronize their actions without sharing the same address space. If two processes want to communicate through messages, a communication link must exist between them. The link and send/receive operations can be implemented, among other ways, using:</p> <ul> <li>Direct or indirect communication</li> <li>Synchronous or asynchronous communication</li> <li>Automatic or explicit buffering</li> </ul> <p>Processes that want to communicate must have a way of referring to each other, by means of direct or indirect communication. Under direct communication each process must explicitly name the recipient or sender of the communication. In this case, the send and receive operations are defined as:</p> <pre><code>send(P, message); // Send a message to process P\nreceive(Q, message); // Receive a message from process Q\n</code></pre> <p>A communication link, in this scheme, has the following properties:</p> <ul> <li>A link is established automatically between every pair of processes, who need to know only each other's identity to communicate.</li> <li>A link associates exactly two processes.</li> <li>Between each pair of processes exists exactly one link.</li> </ul>"},{"location":"summaries/ch3/#references","title":"References","text":"<p>[1] SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition.</p>"},{"location":"summaries/ch4/","title":"Threads","text":""},{"location":"summaries/ch4/#introduction","title":"Introduction","text":"<p>A thread is a basic unit of CPU utilization, and it comprises a thread ID, a program counter, a register set and a stack. It shares with other threads in the same process its code section, data section and operating system resources, such as open files and signals (Figure 1).</p> <p>A heavyweight process is the one that has only one thread of control. A process that has more than one thread can perform more than one task at a time.</p> <p> <p></p> <p> Figure 1: Difference between single threaded and multithreaded processes. Source: [1] </p> <p></p> <p>The major benefits of multithreaded programming are:</p> <ul> <li>Responsiveness: an interactive program can keep running even if a part of it is performing a heavy and lengthy operation.</li> <li>Resource sharing: processes can only share resources through shared memory or message passing, but threads share resources and memory of the process to which they belong by default.</li> <li>Economy: allocating memory and resources for process creation is costly, so it's more economical to create and context-switch threads.</li> <li>Scalability: in a multiprocessor architecture threads may be running in parallel.</li> </ul>"},{"location":"summaries/ch4/#multicore-programming","title":"Multicore Programming","text":"<p>During history, single CPU systems evolved to multi CPU systems and, more recently, to multicore systems. Whether the cores appear across CPU chips or within CPU chips, these systems are called multicore or multiprocessors systems. Multithreading programming provides a mechanism for more efficient use of multicore systems and improved concurrency.</p> <ul> <li>A system is parallel if it can perform more than one task simultaneously;</li> <li>A concurrent system allows more than one task to run by switching between the tasks very rapidly.</li> </ul>"},{"location":"summaries/ch4/#programming-challenges","title":"Programming Challenges","text":"<p>Programming in multicore systems poses five different areas of challenges:</p> <ul> <li>Identifying tasks: this envolves examining applications to find areas that can be divided into separe, concurrent tasks. Ideally tasks are independent from one another and can run in parallel or individual cores.</li> <li>Balance: programmers must ensure that tasks running in parallel contribute equally to the overall process, so that's worth executing them in separe cores.</li> <li>Data splitting: the data accessed and manipulated by the tasks must be divided to run on separate cores.</li> <li>Data dependency: when one task depends on data from another, they must be synchronized to accommodate the data dependency.</li> <li>Testing and debugging: test and debug programs running in parallel on multiple cores is more difficult compared to single-threaded applications debugging.</li> </ul>"},{"location":"summaries/ch4/#types-of-parallelism","title":"Types of parallelism","text":"<p>There are two types of parallelism:</p> <ul> <li>Data parallelism: distributes subsets of the same data across multiple computing cores and all cores perform the same operation.</li> </ul> <p> Summing the elements of the array <code>[0...N]</code> in a dual core system using data parallelism could be implemented by summing <code>[0...N/2-1]</code> in one core and <code>[N/2...N]</code> on the other.</p> <ul> <li>Task parallelism: involves distributing tasks (threads) across multiple cores, each thread performing a unique operation.</li> </ul>"},{"location":"summaries/ch4/#multithreading-models","title":"Multithreading models","text":"<p>Support for threads may be provided at the user level or the kernel level. User threads are supported above the kernel and are managed without kernel support, whereas kernel threads are supported and managed directly by the operating system.</p> <p>A relationship between user and kernel threads must exist, and there are three common ways of establishing this relationship: many-to-one model, one-to-one model and many-to-many model.</p>"},{"location":"summaries/ch4/#many-to-one-model","title":"Many-to-One Model","text":"<p>In this model (Figure 2), many user-level threads are mapped to one kernel thread. The thread management is done by the thread library in user space, so it is efficient. However, the entire process will block if a thread makes a blocking system call. Only one thread can access the kernel at a time, so multiple threads are not able to run in parallel on multicore systems.</p> <p> <p></p> <p> Figure 2: Many-to-One model. Source: [1] </p> <p></p>"},{"location":"summaries/ch4/#one-to-one-model","title":"One-to-One Model","text":"<p>In this model (Figure 3), each user thread is mapped to a kernel thread. It provides more concurrency than the many-to-one model and allows multiple threads to run in parallel on multicore systems. Its only disadvantage is that creating a user thread requires creating the corresponding kernel thread, and the overhead of creating kernel threads can burden the performance of an application. Most implementations of this models restrict the number of threads supported by the system.</p> <p> Linux and Windows operating systems implement the one-to-one model.</p> <p> <p></p> <p> Figure 3: One-to-One model. Source: [1] </p> <p></p>"},{"location":"summaries/ch4/#many-to-many-model","title":"Many-to-Many Model","text":"<p>This model (Figure 4) multiplexes many user-level threads to a smaller or equal number of kernel threads, and this number may be specific to either a particular application or a particular machine. It suffers from neither of the previously mentioned shortcoming, developers can create as many user threads as necessary and the corresponding kernel threads can run in parallel on a multiprocessor. When a thread performs a blocking system call, the kernel can schedule another thread for execution.</p> <p> <p></p> <p> Figure 4: Many-to-Many model. Source: [1] </p> <p></p>"},{"location":"summaries/ch4/#two-level-model","title":"Two-level Model","text":"<p>This model (Figure 5) is a variation of the many-to-many model, in which user threads are still multiplexed to a smaller or equal number of kernel threads, but also can be mapped to a single kernel thread.</p> <p> <p></p> <p> Figure 5: Two-level model. Source: [1] </p> <p></p>"},{"location":"summaries/ch4/#thread-libraries","title":"Thread Libraries","text":"<p>A thread library provides the programmer an API to create and manage threads.</p> <p>There are two primary ways of implementing a thread library, it can be entirely provided in the user level, with no kernel support, or it can be implemented at the kernel level.</p> <p>Thread libraries provided at user level implicates in code and data structure for the library existing in user space, so that invoking a function in this library results in a local function call in user space and not a system call.</p> <p>Kernel-level libraries are supported directly by the operational system, so code and data structure for the library exist in kernel space and invoking a function in the library API usually results in a system call to the kernel.</p> <p>The three main libraries in use today are POSIX Pthreads, Windows and Java. Windows library is a kernel-level available on Windows systems, Pthreads may be provided at user or kernel level, and Java's thread library API is generally implemented using a thread library available on the host system in which the JVM is running.</p> <p>For POSIX and Windows any global data, declared outside of any function, is shared among all threads in the same process. For Java doesn't have the notion of global data, shared access to data must be explicitly arranged between threads. As for local data, it's is usually stored in the stack and since each thread has its own stack, each thread has its copy of local data.</p> <p>Threads can be created by two general strategies: synchronous and asynchronous threading.</p> <ul> <li>Synchronous threading: the parent thread creates one or more children and then must wait for all of its children to terminate before resuming its execution (fork-join strategy). The threads created execute concurrently, but the parent cannot continue until this work is finished.</li> <li>Asynchronous threading: once the parent creates a thread it continues its execution, so that parent and child execute concurrently.</li> </ul>"},{"location":"summaries/ch4/#pthreads","title":"Pthreads","text":"<p>It's a POSIX standard defining an API for thread creation and synchronization, it's a specification, but not an implementation. Operating system designers implement this specification as needed, Linux, MAC OS X and Solaris are some of the UNIX-type systems that implement Pthreads.</p> <pre><code>#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\nint sum;                   // Data shared by the threads.\nvoid *runner(void *param); // Pointer to the function to be shared by the threads.\nint main(int argc, char *argv[])\n{\npthread_t tid;       // Thread identifier.\npthread_attr_t attr; // Set of thread attributes.\nif (argc != 2) {\nfprintf(stderr, \"Usage: a.out &lt;integer value&gt;\\n\");\nreturn -1;\n}\nif (atoi(argv[1]) &lt; 0)\n{\nfprintf(stderr, \"%d must be &gt;=0 \\n\", atoi(argv[1]));\nreturn -1;\n}\npthread_attr_init(&amp;attr); // Initialize thread attributes.\n/* Creates thread identified by tid (passed by reference) to run the \n        runner function with param = argv[1] (passed by reference) */\npthread_create(&amp;tid, &amp;attr, runner, argv[1]);\n/* The calling thread waits for completion of child thread\n            The return status is not stored (NULL param); */\npthread_join(tid, NULL);\nprintf(\"sum = %d\\n\", sum);\nreturn 0;\n}\n// This function is controlled by the thread\nvoid *runner(void *param)\n{\nsum = 0;\nint i, upper = atoi(param);\nfor (i = 1; i &lt;= upper; i++)\nsum += i;\npthread_exit(0);\n}\n</code></pre> <p>To declare multiple threads, one can declare a global variable defining the number of threads and repeat the steps above for all the threads.</p> <pre><code>    #define NUM_THREADS 10\npthread_t workers[NUM_THREADS];\nfor (int i = 0; i &lt; NUM_THREADS; i++)\npthread_join(workers[i], NULL);\n</code></pre>"},{"location":"summaries/ch4/#implicit-threading","title":"Implicit Threading","text":"<p>The growth of multicore processing leads to applications containing hundreds or thousands of threads. Designing such applications brings a lot of challenges to programmers, and one way to address these difficulties is to transfer the creation and management of threads to the compilers and run-time libraries. This strategy is called implicit threading.</p>"},{"location":"summaries/ch4/#thread-pools","title":"Thread Pools","text":"<p>The idea behind a thread pool is to create a number of threads at process startup and place them into a pool, where they sit and wait for work. Once the thread completes its service, it returns to the pool and awaits more work.</p> <p>Thread pools offer these benefits:</p> <ol> <li>Service a request with an existing thread is faster than waiting to create a thread;</li> <li>A thread pool limites the numbe of threads that exist at any one point;</li> <li>Separating the task to be performed from the mechanics of creating the task allow us to use different strategies for running the task, such as schedule the task to execute after a time delay.</li> </ol>"},{"location":"summaries/ch4/#openmp","title":"OpenMP","text":"<p>OpenMP is a set of compiler directives as well as an API for programs written in C, C++ or FORTRAN that provides support for parallel programming in shared-memory environments. OpenMP identifies parallel regions as blocks of code that may run in parallel. Developers insert compiler directives in their code, and this directives instruct the OpenMP run-time library to execute the region in parallel.</p>"},{"location":"summaries/ch4/#grand-central-dispatch","title":"Grand Central Dispatch","text":"<p>It's a technology for Apple's Mac OS X and iOS operating systems, it allows programmers to identify sections of code to run in parallel. Like OpenMP, GCD manages most of the details of threading.</p> <pre><code>^{printf(\"I'm a block\");}\n</code></pre> <p> Blocks are identified by a caret (^) in front a pair of braces {};</p> <p>GCD schedules block for run-time execution by placing them in on a dispatch queue, that can be either serial or concurrent. The blocks placed in the queue are remove in FIFO order, but int the serial queue once a block has been removed, it must complete execution before another block is removed. In concurrent queues several blocks may be removed at a time, allowing them to execute in parallel.</p>"},{"location":"summaries/ch4/#threading-issues","title":"Threading issues","text":""},{"location":"summaries/ch4/#semantics-of-fork-and-exec-system-calls","title":"Semantics of <code>fork()</code> and <code>exec()</code> system calls","text":"<p>The <code>fork()</code> system call is used to create a separate, duplicate process. For multithreaded programs the syntax can differ. Some UNIX systems have two versions of fork, one that duplicates all threads and another that duplicates only the thread that invoked the fork.</p> <p>If a thread invokes the <code>exec()</code> system call, the program specified in the parameter of <code>exec()</code> will replace the entire process, including all threads.</p>"},{"location":"summaries/ch4/#signal-handling","title":"Signal handling","text":"<p>Signals are used to notify a process that a particular event has occurred, and it can be received either synchronously or asynchronously, nonetheless signals follow the same pattern:</p> <ol> <li>A signal is generated by the occurrence of a particular event;</li> <li>The signal is delivered to a process;</li> <li>Once delivered, the signal must be handled.</li> </ol> <p>Synchronous signals are delivered to the same process that performed the operation that caused the signal, and asynchronous are generated by events external to a running process.</p> <p>A signal may be handled by one of two possible handlers:</p> <ol> <li>A default signal handler: every signal has a default handler that the kernel runs when handling it.</li> <li>A user-defined signal handler: the default handler can be overridden by a user defined handler.</li> </ol> <p>Some signals, such as changing the size of a window, are ignored, but others like illegal memory access are handled by terminating the program.</p> <p>When a program is multithreaded, which thread must receive signals? There are some options, and the method vary depending on the type of signal generated. One can:</p> <ol> <li>Deliver the signal to the thread to which the signal applies;</li> <li>Deliver the signal to every thread in the process;</li> <li>Deliver the signal to certain threads in the process;</li> <li>Assign a specific thread to receive all signals for the process;</li> </ol> <p>Synchronous signals need to be delivered to the thread causing the signal, however the situation with asynchronous signals is not as clear. For example, the signal that terminates a process () should be sent to all threads. <p> The standard UNIX function for delivering a signal is: <code>kill (pid_t pid, int signal)</code></p> <p>Most multithreaded versions of UNIX allow a thread to specify which signals it will accept and which it'll block. Because signals need to be handled only once, a signal is typically delivered only to the first thread found that is not blocking it.</p> <p> POSIX Pthreads provides the following function: <code>pthread kill(pthread_t tid, int signal)</code> which delivers a signal to a specified thread (tid).</p>"},{"location":"summaries/ch4/#thread-cancellation","title":"Thread cancellation","text":"<p>It envolves terminating a thread before it has completed, the thread that is to be cancelled is often called the target thread, and it can be cancelled in two different scenarios:</p> <ol> <li>Asynchronous cancellation: One thread immediately terminates the target thread.</li> <li>Deferred cancellation: The target thread periodically checks if it should terminate, so it can properly terminate.</li> </ol> <p>The difficult arises when resources are allocated to a canceled thread or when a thread is cancelled while in the midst of updating data it's sharing with other threads. Cancelling a thread asynchronously may result in some resources not being freed. This doesn't occur with deferred cancellation, as the target thread check a flag to determine if it should be canceled, and then the cancellation occurs safely.</p> <p> Thread cancellation is initiated with the <code>pthread_cancel()</code> in Pthreads.</p> <pre><code>pthread_t tid;\npthread_create(&amp;tid, 0, worker, NULL);\npthread_cancel(tid);\n</code></pre> <p>Pthreads supports three cancellation modes, defined by a state and type.</p> Mode State Type Off Disabled - Deferred Enabled Deferred Asynchronous Enabled Asynchronous <p> Table 1: Cancellation modes in Pthreads. Source: author. </p> <p>The default mode is deferred, and cancellation occurs only when a thread reaches a cancellation point. It can be established by invoking <code>pthread_testcancel()</code>. If a cancellation request is pending, the cleanup handler function is invoked, it allows the resources allocated to this thread to be released before the thread termination.</p> <pre><code>while(1){\n/* \n        Do some work for awhile.\n    */\npthread_testcancel();\n}\n</code></pre> <p> On Linux systems, thread cancellation using Pthreads API is handled through signals.</p>"},{"location":"summaries/ch4/#thread-local-storage","title":"Thread local storage","text":"<p>Threads share data from the process they belong to, but in some cases each thread might need its own copy of certain data, which is called thread-local storage (TLS).</p> <p> TLS aren't the same as local variables, as they area visible across function invocations. TLS are similar to <code>static</code> data, that are declared in the data segment, and not the stack.</p>"},{"location":"summaries/ch4/#scheduler-activations","title":"Scheduler Activations","text":"<p>Communication between the kernel and the thread library may be required by the many-to-many and two-level models discussed previously. Many systems implementing theses models place an intermediate data structure between the user and the kernel threads, known as lightweight process (LWP) (Figure 6). To the user-thread library, the LWP appears to be a virtual processor on which the application can schedule a user thread to run. Each LWP is attached to a kernel thread, and it is schedule to run on physical processors by the operating system. If a kernel thread blocks, it reflects up to the LWP and the user-level thread and they also block.</p> <p> </p> <p> Figure 6: Lightweight process. Source: [1] </p> <p>An application may require any number of LWPs to run. Typically, a LWP is required for each concurrent blocking system call, for example, if five concurrent file-read requests occur simultaneously, five LWPs are needed.</p> <p>The communication between kernel and user-thread library can be done by a scheduler activation. In this scheme, the kernel provides an application with a set of virtual processors (LWPs), and the application can schedule user threads onto an available virtual processor. The kernel must inform an application about certain events, in a procedure known as upcall. They are handled by the thread library, with an upcall handler, which runs in a virtual processor. When an application thread is about to block, the kernel makes an upcall to the application informing it and identifying the blocking thread. The kernel allocates a new virtual processor to the application, which runs an upcall handler on this new processor, saving the state of the blocking thread and relinquishes the virtual processor where the blocking thread is running. The upcall handler schedules another thread to run on the new virtual processor. When the event the blocking thread was waiting for happens, the kernel makes another upcall to the thread library informing that the blocking thread is now eligible to run. Since the upcall handler for this second event also needs a virtual processor, the kernel makes this allocation to a new virtual processor or one of those being used by the user threads.</p>"},{"location":"summaries/ch4/#references","title":"References","text":"<p>[1] SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition.</p>"},{"location":"summaries/ch5/","title":"Process Synchronization","text":""},{"location":"summaries/ch5/#background","title":"Background","text":"<p>It's known that processes can execute concurrently, be interrupted at any time and partially complete execution. Because of this, concurrent access to shared data may lead to data inconsistency. A situation where several processes access and manipulate the same data concurrently and the outcome of the execution dependes on the particular order of access is called race condition. To avoid this problem, the operating system must provide mechanisms to ensure the orderly execution of cooperating processes.</p>"},{"location":"summaries/ch5/#the-critical-section-problem","title":"The Critical Section Problem","text":"<p>When sequential processes are sharing data, mutual exclusion must be provided so that a critical section of code is used by only one process or thread at a time. Each process must ask permission to enter critical section in entry section, may follow critical section with exit section then remainder section.</p> <p> <p> </p> <p> Figure 1: Structure of critical section. Source: [1] </p> <p>A good solution for this problem must ensure:</p> <ol> <li>Mutual Exclusion: if process \\(P_i\\) is executing its critical section, then no other process can execute its critical section.</li> <li>Progress: if no process is executing its critical section and there exists processes waiting to enter their critical section, then the selection of the processes that will enter the critical section next can't be postponed indefinitely.</li> <li>Bounded waiting: a bound must exist on the number of times that other processes enter their critical sections after a process has made a request to enter its critical section and before that request is granted.</li> </ol> <p> We assume that each process executes at a nonzero speed, but no assumption is made about the relative speed of the processes.</p> <p>Two general approaches are used to handle critical sections in operating systems:</p> <ul> <li>Preemptive kernels. Allows a process to be preempted while it's running in kernel mode.</li> <li>Non-preemptive kernels. Doesn't allow a process running in kernel mode to be preempted, so a kernel-mode process will run until it exits kernel mode, blocks or voluntarily yields control of the CPU. Free of race conditions in kernel mode.</li> </ul>"},{"location":"summaries/ch5/#petersons-solution","title":"Peterson's Solution","text":"<p>Two processes share two variables, <code>turn</code> and <code>flag</code>. The variable <code>turn</code> indicates whose turn it is to enter the critical section. The <code>flag</code> array is used to indicate if a process is ready to enter the critical section, i. e., <code>flag[i]=true</code> indicates that process \\(P_i\\) is ready.</p> <pre><code>int turn;\nbool flag[2];\nvoid process_0(){\ndo {\nflag[0] = true;\nturn = j;\nwhile (flag[j] &amp;&amp; turn == j){\n// Critical section\n}\nflag[i] = false;\n// remainder section\n} while (true);\n}\n</code></pre> <p>This solution meets all the CS requirements.</p> <ol> <li>Mutual exclusion is preserved, because \\(P_i\\) enters CS only if either <code>flag[j] = false</code> or <code>turn = i</code>.</li> <li>Progress requirement is satisfied.</li> <li>Bounded-waiting requirement is met.</li> </ol>"},{"location":"summaries/ch5/#synchronization-hardware","title":"Synchronization Hardware","text":"<p>Many systems provide hardware support for implement the critical section code. The solutions below are base on the idea of locking, that is, protecting critical regions via locks.</p> <p>In a single-processor environment, the critical-section problem could be solved by preventing interrupts from occurring while a shared variable was being modified, in this way we would assure the instructions' order of execution, without preemption. This is often the approach used in nonpreemptive kernels. However, disabling interruptions in a multiprocessor environment can be time consuming and it would decrease system efficiency.</p> <p>Many modern computer systems provide special hardware instructions to test and modify the content of a word or to swap the contents of two words atomically, that is, as one uninterruptible unit.</p> <p>The <code>test_and_set()</code> instruction can be defined as shown below, and it's executed atomically. Thus, if two test and set instructions are executed simultaneously (on different CPUs), they will be executed sequentially in some arbitrary order.</p> <pre><code>bool test_and_set(bool *target){\nbool rv = *target;\n*target = true;\nreturn rv;\n}\n</code></pre> <p>If the machine supports the test and set instruction, then one can implement mutual exclusion by declaring a bool variable <code>lock</code>, initialized to false. An example of code to the ith process is shown below.</p> <pre><code>do {\n// While lock == true, do nothing\nwhile (test_and_set(&amp;lock)){} // Begin of critical section, as soon as lock = false;\nlock = false; // Releases the lock.\n// Remainder section\n} while (true);\n</code></pre> <p>The compare and swap instruction changes a variable value to a new value only if its current value equals a expected value, and always return the original value of the variable. It's also executed atomically.</p> <pre><code>int compare_and_swap(int *value, int expected, int new_value){\nint temp = *value;\nif (*value == expected){\n*value = new_value;\n}\nreturn temp;\n}\n</code></pre> <p>Mutual exclusion can be provided by defining a global variable lock, initialized to 0. The first process that invokes <code>compare_and_swap()</code> will set lock to 1. Then it enter its critical section, and after finished it releases the lock.</p> <pre><code>do {\nwhile (compare_and_swap(&amp;lock, 0, 1) != 0){} // Do nothing until acquires lock.\n// Critical section\nlock = 0; // Releases lock.\n// Remainder section.\n} while (true);\n</code></pre> <p>These algorithms satisfy mutual-exclusion, but they do not satisfy the bounded-waiting requirement, which the algorithm below solves.</p> <pre><code>// Shared structures\nbool *waiting;\nbool lock;\nvoid *process_i(void *adress_i)\n{\nbool key;\nint j; // next process to acquire the lock\ndo\n{\nwaiting[i] = true;\nkey = true;\nwhile (waiting[i] &amp;&amp; key)\n// When key == false, the loop ends.\nkey = test_and_set(&amp;lock);\nwaiting[i] = false; // Exits the waiting line.\n// Critical section.\nj = (i + 1) % n; // Circular buffer.\nwhile ((j != 1) &amp;&amp; !waiting[j])\n// Circle through processes searching for one that's waiting\nj = (i + j) % n;\nif (j == i)\nlock = false; // Releases its lock\nelse\nwaiting[j] = false; // Signal for process_j to begin\n// Remainder section;\n} while (true);\n}\n</code></pre> <p>This algorithm satisfies the three conditions:</p> <ol> <li>Mutual exclusion, implemented by the locks.</li> <li>Progress, as no process is kept waiting while the lock is free.</li> <li>Bounded-waiting, for one process gives the lock to the following in a circular order.</li> </ol>"},{"location":"summaries/ch5/#mutex-locks","title":"Mutex Locks","text":"<p>The hardware-based solutions are generally inaccessible to application programmers. Operating-systems designers build software tools to solve the critical-section problem. The simplest one is the mutex lock. It's used to protect critical regions and thus prevent race conditions. A process must acquire the lock before entering a critical section and releases the lock at section's completion.</p> <pre><code>acquire(){\nwhile (!available); // Busy wait.\navailable = false;\n}\nrelease(){\navailable = true;\n}\n</code></pre> <p>If the is available, a call to <code>acquire()</code> succeeds and the lock gets unavailable. If the lock isn't available, a process that attempts to acquire the lock is blocked until the lock is released. Calls to <code>acquire()</code> and <code>release()</code> must be performed atomically.</p> <p>The main disadvantage of this implementation, is that it requires busy waiting. This type of mutex is also called spinlock, because a process \"spins\" while waiting for the lock. Spinlocks have the advantage of avoiding context switch, and they're useful when the locks will be held for short periods of time. They're often employed on multiprocessor systems, where one thread can \"spin\" on one processor while another performs its critical section on another processor.</p>"},{"location":"summaries/ch5/#semaphores","title":"Semaphores","text":"<p>A semaphore S is an integer variable that, after initialized, is accessed only through two atomic operations: <code>wait()</code> and <code>signal()</code>.</p> <pre><code>    wait(S){\nwhile (S&lt;=0)\n; // Busy wait\nS--;\n}\nsignal(S){\nS++;\n}\n</code></pre> <p>The operations in wait and signal must be executed indivisibly, i. e., when one process modifies the semaphore value, no other process can simultaneously modify that same semaphore value. In the case of wait(S), the testing and the possible modification must be executed without interruption.</p>"},{"location":"summaries/ch5/#semaphore-usage","title":"Semaphore Usage","text":"<p>Operating systems often distinguish between counting semaphores, which value ranges over an unrestricted domain, and binary semaphores, ranging between 0 and 1, behaving similarly to mutex locks.</p> <p>Counting semaphores can be used to control access to a given resource with a finite number of instances. In this case, the semaphore is initialized to the number of resources available, and every process that wishes to use the resource performs a wait. If no instance is available, the process will block until another one releases the resource.</p> <p>Semaphores can also be used to assure order of execution in concurrent processes, as follows:</p> <pre><code>    // Initialize semaphore to zero.\nint S = 0; process_1(){\nstatement_1;\nsignal(S);\n}\nprocess_2(){ // Statement 2 must execute after statement 1.\nwait(S);\nstatement_2;\n}\n</code></pre>"},{"location":"summaries/ch5/#semaphore-implementation","title":"Semaphore Implementation","text":"<p>The busy waiting can be avoided by making a process block itself if <code>S&lt;=0</code>. The block operation puts a process into a waiting queue associated with the semaphore, and the state of the process is changed to waiting. Then, the control is transferred to the CPU scheduler, which selects another process to execute.</p> <p>A process that is blocked, waiting on a semaphore S, should be restarted (through the <code>wakeup()</code> operation) when other process executes a <code>signal()</code> operation.</p> <pre><code>typedef struct {\nint value;\n// List of waiting processes.\nstruct process *list; } semaphore;\nwait (semaphore *S){\nS-&gt;value--;\nif (S-&gt;value &lt;0) {\nadd this process to S-&gt; list;\n/* \n        Suspends the process that invokes it.\n        Provided by the OS as system call.\n        */\nblock();\n}\n}\nsignal(semaphore *S){\nS-&gt;value++;\nif (S-&gt; value &lt;=0){\nremoves a process P from S-&gt;list;\n/* \n        Wakes a process that was blocked. \n        Provided by the OS as system call.\n        */\nwakeup(P); }\n}\n</code></pre> <p> In this implementations, semaphores may be negative, in this case the magnitude of the semaphore is the number of processes waiting on it.</p> <p>The list of waiting processes can be implemented as a list of PCBs (process control block), each semaphore contains an integer value and a pointer to this list. To ensure bounded waiting, the list can be implemented as a FIFO queue.</p>"},{"location":"summaries/ch5/#deadlocks","title":"Deadlocks","text":"<p>A deadlock state occurs when two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes. They can be dealt with by three principal methods:</p> <ol> <li>Use some protocol to prevent deadlocks, ensuring that the system will never enter a deadlocked state.</li> <li>Allow the system to enter a deadlocked state, detect it and recover.</li> <li>Ignore the problem altogether and pretend that deadlocks never occur in the system (used by most of the OSs).</li> </ol> <p>A deadlock can occur only if four conditions hold simultaneously in the system: mutual exclusion, hold and wait, no preemption and circular wait.</p>"},{"location":"summaries/ch5/#starvation","title":"Starvation","text":"<p>Also called indefinite blocking, starvation happens when a process waits indefinitely for a resource. It may occur in semaphores if the processes are removed from the wait list in a LIFO (last-in, first-out) order.</p>"},{"location":"summaries/ch5/#priority-inversion","title":"Priority Inversion","text":"<p>Priority inversion happens when a low priority process interferes in how long a high priority process may wait to execute, and it can only occur in system with more than two priorities.</p> <p> Example: Assume we have three processes A, B and C, with priority C &lt; B &lt; A. Assume that process A requires a resource R, which is being accessed by process C, so A must wait for C releasing the resource. In the meantime, B may become runnable and preempt process A, causing an inversion of priority.</p> <p>This problem can be solved by having only two priorities in the system. However, this is insufficient for most operating systems, so they can solve this problem implementing a priority-inheritance protocol. All processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources in question. When they're finished, their priorities reverts to the original values.</p> <p> In the example above, the protocol would allow process C to temporarily inherit the priority of process A, preventing process B to preempting its execution. When C had finished using the resource R, it would come back to its original priority and A would execute, because resource R is now available.</p>"},{"location":"summaries/ch5/#classic-problems-of-synchronization","title":"Classic Problems of Synchronization","text":"<p>Various synchronization problems, such as the bounded-buffer, readers-writers and dining-philosophers problems, are important mainly because they're examples of larger classes of concurrency-control problems. They are used to test nearly every newly proposed synchronization scheme.</p>"},{"location":"summaries/ch5/#the-bounded-buffer-problem","title":"The Bounded-Buffer Problem","text":"<p>In this problem, producer and consumer processes share the following data structures:</p> <pre><code>    int n;\nsemaphore mutex = 1;\nsemaphore empty = n;\nsemaphore full = 0;\nvoid producer_process(){\ndo {\n// Produce an iten in next_produced.\nwait(empty);\nwait(mutex);\n// Add next_produced to buffer.\nsignal(mutex);\nsignal(full);\n} while (true);\n}\nvoid consumer_process(){\ndo {\nwait(full);\nwait(mutex);\n// Remove and item from buffer to next_consumed.\nsignal(mutex);\nsignal(empty);\n// Consume the item in next_consumed.\n} while (true);\n}\n</code></pre> <p>We assume the pool consists of n buffers capable of holding one item. The mutex semaphore manages access to the buffer pool. The empty and full semaphores count the number of empty and full buffers.</p> <p>In this code, we can say that the producer is producing full buffers for the consumer or that the consumer is producing empty buffers for the producer.</p>"},{"location":"summaries/ch5/#the-readers-writers-problem","title":"The Readers-Writers Problem","text":"<p>Processes that want only to read data are called readers and the ones that want to read and write are called writers. More than one reader can access data simultaneously, but only one writer can access data at a time.</p> <p>This problem has some variations, the first version requires that no reader be kept waiting unless a writer has already obtained permission to use the shared object.</p>"},{"location":"summaries/ch5/#the-dining-philosophers-problem","title":"The Dining-Philosophers Problem","text":""},{"location":"summaries/ch5/#alternative-approaches","title":"Alternative approaches","text":""},{"location":"summaries/ch5/#references","title":"References","text":"<p>[1] SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition.</p>"},{"location":"summaries/ch9/","title":"Chapter 9","text":""},{"location":"summaries/ch9/#overview","title":"Overview","text":"<p>Disk drives are the major secondary storage I/O devices on most computers. Most secondary storage devices are either magnetic tapes or hard disks, but solid-state disks are becoming more popular.</p>"},{"location":"summaries/ch9/#hard-disks","title":"Hard disks","text":"<p>Hard disks (Figures 1 and 2) provide the bulk of secondary storage for modern system computers. Each disk platter has a flat circular shape, like a CD, with diameter ranging from 4,5 to 8,9 cm. The two surfaces of the platter are covered with a magnetic material, and information is recorded magnetically on the platters.</p> <p> <p> </p> <p> Figure 1: Hard disk drive. Source: [2] </p> <p>A read-write head is above each surface of every platter, and are attached to a disk arm that moves all the heads as a unit. The surface of a platter is logically divided into circular tracks, which are subdivided into sectors. The set of tracks that are at one arm position forms a cylinder.</p> <p> <p> </p> <p> Figure 2: Moving-head disk mechanism. Source: [1] </p> <p>When the disk is in use, a motor spins it at high speed. Disk speed has two parts: the transfer rate and the random-access time (positioning time). The transfer rage is the rate that data flows between the drive and the computer, and random-access time consists of the time necessary to move the disk arm to the desired cylinder (seek time) and the time necessary for the desired sector rotate to the disk head (rotational latency).</p> <p> Common drives usually:</p> <ul> <li>spin at 5,400, 7,200, 10,000 and 15,000 RPM.</li> <li>can transfer several megabytes of data per second and have seek times and rotational latencies of several milliseconds.</li> </ul> <p>The disk head flies on a very thin space, measured in microns, so there's a danger that the head will make contact with the disk surface. Even though the disk platters are coated with a thin protective layer, the head will sometimes damage the magnetic surface, and this accident is called head crash. A head crash normally cannot be repaired, requiring the replacement of the entire disk.</p> <p>A disk can be removable, allowing different disks to be mounted as needed. Removable hard disks usually consist of one platter, held in a plastic case to prevent damage.</p> <p> CDs, DVDs, Blu-rays discs and removable flash-memory are other forms of removable disks.</p> <p>A disk drive is attached to a computer by a set of wires called an I/O bus. The data transfers on a bus are carried out by special electronic processors called controllers, the host controller is at the computer end of the bus and the disk controller is built into each disk drive. To perform a disk I/O operation, the computer places a command into the host controller, typically using memory-mapped I/O ports. The host controller then sends the command via messages to the disk controller, and it operates the disk-drive hardware to carry out the command. Disk controllers usually have a built-in cache. Data transfer at the disk drive happens between the cache and the disk surface. Data transfer to the host occurs between the cache and the host controller.</p> <p> Several kind of buses are available, including advanced technology attachment (ATA), serial ATA (SATA), eSATA, universal serial bus (USB) and fibre channel (FC).</p>"},{"location":"summaries/ch9/#solid-state-disks","title":"Solid-State Disks","text":"<p>Sometimes old technologies are used in new ways as economics change or the technologies evolve. An example is the growing importance of solid-state disks, or SSDs. Simply described, an SSD is nonvolatile memory that is used like a hard drive. There are many variations of this technology, from DRAM with a battery to allow it to maintain its state in a power failure through flash-memory technologies like single-level cell (SLC) and multilevel cell (MLC) chips.</p> <p>SSDs have the same characteristics as traditional hard disks but can be more reliable because they have no moving parts and faster because they have no seek time or latency. In addition, they consume less power. However, they are more expensive per megabyte than traditional hard disks, have less capacity than the larger hard disks, and may have shorter life spans than hard disks, so their uses are somewhat limited. One use for SSDs is in storage arrays, where they hold file-system metadata that require high performance. SSDs are also used in some laptop computers to make them smaller, faster, and more energy-efficient.</p> <p>Because SSDs can be much faster than hard disk drives, standard bus interfaces can cause a major limit on throughput. Some SSDs are designed to connect directly to the system bus (PCI, for example). SSDs are changing other traditional aspects of computer design as well. Some systems use them as a direct replacement for disk drives, while others use them as a new cache tier, moving data between hard disks, SSDs, and memory to optimize performance.</p>"},{"location":"summaries/ch9/#magnetic-tapes","title":"Magnetic Tapes","text":"<p>Magnetic tape was used as an early secondary-storage medium. Although it is relatively permanent and can hold large quantities of data, its access time is slow compared with that of main memory and hard disk. In addition, random access to magnetic tape is about a thousand times slower than random access to hard disk, so tapes are not very useful for secondary storage. Tapes are  used mainly for backup, for storage of infrequently used information, and as a medium for transferring information from one system to another.</p> <p>A tape is kept in a spool and is wound or rewound past a read \u2013write head. Moving to the correct spot on a tape can take minutes, but once positioned, tape drives can write data at speeds comparable to disk drives. Tape capacities vary greatly, depending on the particular kind of tape drive, with current capacities exceeding several terabytes. Some tapes have built-in compression that can more than double the effective storage. Tapes and their drivers are usually categorized by width, including 4, 8, and 19 millimeters and 1/4 and 1/2 inch. Some are named according to technology, such as LTO-5 and SDLT.</p>"},{"location":"summaries/ch9/#disk-structure","title":"Disk Structure","text":"<p>Modern disk drives are structured as large one-dimensional arrays of logical disk blocks, usually having 512 bytes of size.</p>"},{"location":"summaries/ch9/#disk-attachment","title":"Disk Attachment","text":"<p>Disks may be attached to a computer system in one of two ways:</p> <ul> <li>Through the local I/O ports on the host computer</li> <li>Through a network connection</li> </ul>"},{"location":"summaries/ch9/#disk-scheduling","title":"Disk Scheduling","text":"<p>Disk scheduling algorithms aim to maximize performance by scheduling the order of disk I/O.</p> <p>Requests for disk I/O are generated by the file system and by the virtual memory system. Each request specifies the address on the disk to be referenced, in the form of a logical block number. Disk-scheduling algorithms can improve the effective bandwidth, the average response time and the variance in response time.</p> <p>Algorithms such as SSTF, SCAN, C-SCAN, LOOK and C-LOOK are designed to make such improvements through strategies for disk-queue ordering.</p> <p>Performance of disk-scheduling algorithms can vary greatly on hard disks. However, because solid-state disks have no moving parts, performance varies little among algorithms, and quite often a simple FCFS strategy is used.</p>"},{"location":"summaries/ch9/#swap-space-management","title":"Swap-Space Management","text":""},{"location":"summaries/ch9/#raid-structure","title":"RAID Structure","text":""},{"location":"summaries/ch9/#stable-storage-implementation","title":"Stable-Storage Implementation","text":""},{"location":"summaries/ch9/#references","title":"References","text":"<p>[1] SILBERSCHATZ A., GALVIN P., GAGNE G. Operating System Concepts, 9th Edition. [2] https://blocksandfiles.com/2019/10/07/10-platter-hard-disk-drives/</p>"}]}